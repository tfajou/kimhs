{
    "Import TensorFlow": {
        "prefix": "tf:import",
        "body": [
            "import tensorflow as tf"
        ],
        "description": "Import TensorFlow package"
    },
    "2D Convolution": {
        "prefix": "tf:conv2d",
        "body": [
            "${1:features} = tf.layers.conv2d(",
            "\t${1:features},",
            "\tfilters=${2:64},",
            "\tkernel_size=${3:3},",
            "\tpadding=\"${4:same}\",",
            "\tname=\"conv2d/${5:1}\")"
        ],
        "description": "2D Convolution"
    },
    "Transposed 2D Convolution": {
        "prefix": "tf:transposed-conv2d",
        "body": [
            "${1:features} = tf.layers.conv2d_transpose(",
            "\t${1:features},",
            "\tfilters=${2:64},",
            "\tkernel_size=${3:3},",
            "\tpadding=\"${4:same}\",",
            "\tname=\"conv2d_transpose/${5:1}\")"
        ],
        "description": "Transposed 2D Convolution"
    },
    "Separable 2D Convolution": {
        "prefix": "tf:separable-conv2d",
        "body": [
            "${1:features} = tf.layers.separable_conv2d(",
            "\t${1:features},",
            "\tfilters=${2:64},",
            "\tkernel_size=${3:3},",
            "\tpadding=\"${4:same}\",",
            "\tname=\"conv2d_separable/${5:1}\")"
        ],
        "description": "Separable 2D Convolution"
    },
    "Dense layer": {
        "prefix": "tf:dense",
        "body": [
            "${1:features} = tf.layers.dense(${1:features}, units=${2:64}, name=\"dense/${3:1}\")"
        ],
        "description": "Dense layer"
    },
    "2D max pooling layer": {
        "prefix": "tf:max-pool2d",
        "body": [
            "${1:features} = tf.layers.max_pooling2d(",
            "\t${1:features}, pool_size=${2:2}, strides=${2:2}, padding=\"${3:same}\")"
        ],
        "description": "2D max pooling layer"
    },
    "Dropout layer": {
        "prefix": "tf:dropout",
        "body": [
            "${1:features} = tf.layers.dropout(${1:features}, rate=${2:0.5})"
        ],
        "description": "Dropout layer"
    },
    "Batch normalization layer": {
        "prefix": "tf:batch-norm",
        "body": [
            "${1:features} = tf.layers.batch_normalization(${1:features})"
        ],
        "description": "Batch normalization layer"
    },
    "Softmax cross entropy loss": {
        "prefix": "tf:cross-entropy",
        "body": [
            "loss = tf.losses.sparse_softmax_cross_entropy(",
            "\tlabels=${1:labels}, logits=${2:logits})"
        ],
        "description": "Softmax cross entropy loss"
    },
    "ResNet block": {
        "prefix": "tf:resnet-block",
        "body": [
            "def resnet_block(features, bottleneck, out_filters, training):",
            "\t\"\"\"Residual block.\"\"\"",
            "\twith tf.variable_scope(\"input\"):",
            "\t\toriginal = features",
            "\t\tfeatures = tf.layers.conv2d(features, bottleneck, 1, activation=None)",
            "\t\tfeatures = tf.layers.batch_normalization(features, training=training)",
            "\t\tfeatures = tf.nn.relu(features)",
            "",
            "\twith tf.variable_scope(\"bottleneck\"):",
            "\t\tfeatures = tf.layers.conv2d(",
            "\t\t\tfeatures, bottleneck, 3, activation=None, padding=\"same\")",
            "\t\tfeatures = tf.layers.batch_normalization(features, training=training)",
            "\t\tfeatures = tf.nn.relu(features)",
            "",
            "\twith tf.variable_scope(\"output\"):",
            "\t\tfeatures = tf.layers.conv2d(features, out_filters, 1)",
            "\t\tin_dims = original.shape[-1].value",
            "\t\tif in_dims != out_filters:",
            "\t\t\toriginal = tf.layers.conv2d(features, out_filters, 1, activation=None,",
            "\t\t\t\tname=\"proj\")",
            "\t\tfeatures += original",
            "\treturn features"
        ],
        "description": "ResNet block"
    },
    "TensorFlow trainer": {
        "prefix": "tf:trainer",
        "body": [
            "\"\"\"This module handles training and evaluation of a neural network model.",
            "",
            "Invoke the following command to train the model:",
            "python -m trainer --model=cnn --dataset=mnist",
            "",
            "You can then monitor the logs on Tensorboard:",
            "tensorboard --logdir=output\"\"\"",
            "",
            "from __future__ import absolute_import",
            "from __future__ import division",
            "from __future__ import print_function",
            "",
            "import tensorflow as tf",
            "",
            "tf.logging.set_verbosity(tf.logging.INFO)",
            "",
            "tf.flags.DEFINE_string(\"model\", \"\", \"Model name.\")",
            "tf.flags.DEFINE_string(\"dataset\", \"\", \"Dataset name.\")",
            "tf.flags.DEFINE_string(\"output_dir\", \"\", \"Optional output dir.\")",
            "tf.flags.DEFINE_string(\"schedule\", \"train_and_evaluate\", \"Schedule.\")",
            "tf.flags.DEFINE_string(\"hparams\", \"\", \"Hyper parameters.\")",
            "tf.flags.DEFINE_integer(\"num_epochs\", 100000, \"Number of training epochs.\")",
            "tf.flags.DEFINE_integer(\"save_summary_steps\", 10, \"Summary steps.\")",
            "tf.flags.DEFINE_integer(\"save_checkpoints_steps\", 10, \"Checkpoint steps.\")",
            "tf.flags.DEFINE_integer(\"eval_steps\", None, \"Number of eval steps.\")",
            "tf.flags.DEFINE_integer(\"eval_frequency\", 10, \"Eval frequency.\")",
            "",
            "FLAGS = tf.flags.FLAGS",
            "",
            "MODELS = {",
            "\t# This is a dictionary of models, the keys are model names, and the values",
            "\t# are the module containing get_params, model, and eval_metrics.",
            "\t# Example: \"cnn\": cnn",
            "}",
            "",
            "DATASETS = {",
            "\t# This is a dictionary of datasets, the keys are dataset names, and the",
            "\t# values are the module containing get_params, prepare, read, and parse.",
            "\t# Example: \"mnist\": mnist",
            "}",
            "",
            "HPARAMS = {",
            "\t\"optimizer\": \"Adam\",",
            "\t\"learning_rate\": 0.001,",
            "\t\"decay_steps\": 10000,",
            "\t\"batch_size\": 128",
            "}",
            "",
            "def get_params():",
            "\t\"\"\"Aggregates and returns hyper parameters.\"\"\"",
            "\thparams = HPARAMS",
            "\thparams.update(DATASETS[FLAGS.dataset].get_params())",
            "\thparams.update(MODELS[FLAGS.model].get_params())",
            "",
            "\thparams = tf.contrib.training.HParams(**hparams)",
            "\thparams.parse(FLAGS.hparams)",
            "",
            "\treturn hparams",
            "",
            "def make_input_fn(mode, params):",
            "\t\"\"\"Returns an input function to read the dataset.\"\"\"",
            "\tdef _input_fn():",
            "\t\tdataset = DATASETS[FLAGS.dataset].read(mode)",
            "\t\tif mode == tf.estimator.ModeKeys.TRAIN:",
            "\t\t\tdataset = dataset.repeat(FLAGS.num_epochs)",
            "\t\t\tdataset = dataset.shuffle(params.batch_size * 5)",
            "\t\tdataset = dataset.map(",
            "\t\t\tDATASETS[FLAGS.dataset].parse, num_threads=8)",
            "\t\tdataset = dataset.batch(params.batch_size)",
            "\t\titerator = dataset.make_one_shot_iterator()",
            "\t\tfeatures, labels = iterator.get_next()",
            "\t\treturn features, labels",
            "\treturn _input_fn",
            "",
            "def make_model_fn():",
            "\t\"\"\"Returns a model function.\"\"\"",
            "\tdef _model_fn(features, labels, mode, params):",
            "\t\tmodel_fn = MODELS[FLAGS.model].model",
            "\t\tglobal_step = tf.train.get_or_create_global_step()",
            "\t\tpredictions, loss = model_fn(features, labels, mode, params)",
            "",
            "\t\ttrain_op = None",
            "\t\tif mode == tf.estimator.ModeKeys.TRAIN:",
            "\t\t\tdef _decay(learning_rate, global_step):",
            "\t\t\t\tlearning_rate = tf.train.exponential_decay(",
            "\t\t\t\t\tlearning_rate, global_step, params.decay_steps, 0.5,",
            "\t\t\t\t\tstaircase=True)",
            "\t\t\t\treturn learning_rate",
            "",
            "\t\t\ttrain_op = tf.contrib.layers.optimize_loss(",
            "\t\t\t\tloss=loss,",
            "\t\t\t\tglobal_step=global_step,",
            "\t\t\t\tlearning_rate=params.learning_rate,",
            "\t\t\t\toptimizer=params.optimizer,",
            "\t\t\t\tlearning_rate_decay_fn=_decay)",
            "",
            "\t\treturn tf.contrib.learn.ModelFnOps(",
            "\t\t\tmode=mode,",
            "\t\t\tpredictions=predictions,",
            "\t\t\tloss=loss,",
            "\t\t\ttrain_op=train_op)",
            "",
            "\treturn _model_fn",
            "",
            "def experiment_fn(run_config, hparams):",
            "\t\"\"\"Constructs an experiment object.\"\"\"",
            "\testimator = tf.contrib.learn.Estimator(",
            "\t\tmodel_fn=make_model_fn(), config=run_config, params=hparams)",
            "\treturn tf.contrib.learn.Experiment(",
            "\t\testimator=estimator,",
            "\t\ttrain_input_fn=make_input_fn(tf.estimator.ModeKeys.TRAIN, hparams),",
            "\t\teval_input_fn=make_input_fn(tf.estimator.ModeKeys.EVAL, hparams),",
            "\t\teval_metrics=MODELS[FLAGS.model].eval_metrics(hparams),",
            "\t\teval_steps=FLAGS.eval_steps,",
            "\t\tmin_eval_frequency=FLAGS.eval_frequency)",
            "",
            "def main(unused_argv):",
            "\t\"\"\"Main entry point.\"\"\"",
            "\tif FLAGS.output_dir:",
            "\t\tmodel_dir = FLAGS.output_dir",
            "\telse:",
            "\t\tmodel_dir = \"output/%s_%s\" % (FLAGS.model, FLAGS.dataset)",
            "",
            "\tDATASETS[FLAGS.dataset].prepare()",
            "",
            "\tsession_config = tf.ConfigProto()",
            "\tsession_config.allow_soft_placement = True",
            "\tsession_config.gpu_options.allow_growth = True",
            "\trun_config = tf.contrib.learn.RunConfig(",
            "\t\tmodel_dir=model_dir,",
            "\t\tsave_summary_steps=FLAGS.save_summary_steps,",
            "\t\tsave_checkpoints_steps=FLAGS.save_checkpoints_steps,",
            "\t\tsave_checkpoints_secs=None,",
            "\t\tsession_config=session_config)",
            "",
            "\ttf.contrib.learn.learn_runner.run(",
            "\t\texperiment_fn=experiment_fn,",
            "\t\trun_config=run_config,",
            "\t\tschedule=FLAGS.schedule,",
            "\t\thparams=get_params())",
            "",
            "if __name__ == \"__main__\":",
            "\ttf.app.run()",
            ""
        ],
        "description": "A trainer module based on tf.contrib.learn API."
    },
    "MNIST dataset" : {
        "prefix": "tf:mnist",
        "body": [
            "\"\"\"MNIST dataset preprocessing and specifications.\"\"\"",
            "",
            "from __future__ import absolute_import",
            "from __future__ import division",
            "from __future__ import print_function",
            "",
            "import gzip",
            "import numpy as np",
            "import os",
            "from six.moves import urllib",
            "import struct",
            "import tensorflow as tf",
            "",
            "REMOTE_URL = \"http://yann.lecun.com/exdb/mnist/\"",
            "LOCAL_DIR = \"data/mnist/\"",
            "TRAIN_IMAGE_URL = \"train-images-idx3-ubyte.gz\"",
            "TRAIN_LABEL_URL = \"train-labels-idx1-ubyte.gz\"",
            "TEST_IMAGE_URL = \"t10k-images-idx3-ubyte.gz\"",
            "TEST_LABEL_URL = \"t10k-labels-idx1-ubyte.gz\"",
            "",
            "IMAGE_SIZE = 28",
            "NUM_CLASSES = 10",
            "",
            "def get_params():",
            "\t\"\"\"Dataset params.\"\"\"",
            "\treturn {",
            "\t\t\"num_classes\": NUM_CLASSES,",
            "\t}",
            "",
            "def prepare():",
            "\t\"\"\"This function will be called once to prepare the dataset.\"\"\"",
            "\tif not os.path.exists(LOCAL_DIR):",
            "\t\tos.makedirs(LOCAL_DIR)",
            "\tfor name in [",
            "\t\t\tTRAIN_IMAGE_URL,",
            "\t\t\tTRAIN_LABEL_URL,",
            "\t\t\tTEST_IMAGE_URL,",
            "\t\t\tTEST_LABEL_URL]:",
            "\t\tif not os.path.exists(LOCAL_DIR + name):",
            "\t\t\turllib.request.urlretrieve(REMOTE_URL + name, LOCAL_DIR + name)",
            "",
            "def read(split):",
            "\t\"\"\"Create an instance of the dataset object.\"\"\"",
            "\timage_urls = {",
            "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_IMAGE_URL,",
            "\t\ttf.estimator.ModeKeys.EVAL: TEST_IMAGE_URL",
            "\t}[split]",
            "\tlabel_urls = {",
            "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_LABEL_URL,",
            "\t\ttf.estimator.ModeKeys.EVAL: TEST_LABEL_URL",
            "\t}[split]",
            "",
            "\twith gzip.open(LOCAL_DIR + image_urls, \"rb\") as f:",
            "\t\tmagic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))",
            "\t\timages = np.frombuffer(f.read(num * rows * cols), dtype=np.uint8)",
            "\t\timages = np.reshape(images, [num, rows, cols, 1])",
            "\t\tprint(\"Loaded %d images of size [%d, %d].\" % (num, rows, cols))",
            "",
            "\twith gzip.open(LOCAL_DIR + label_urls, \"rb\") as f:",
            "\t\tmagic, num = struct.unpack(\">II\", f.read(8))",
            "\t\tlabels = np.frombuffer(f.read(num), dtype=np.int8)",
            "\t\tprint(\"Loaded %d labels.\" % num)",
            "",
            "\treturn tf.contrib.data.Dataset.from_tensor_slices((images, labels))",
            "",
            "def parse(image, label):",
            "\t\"\"\"Parse input record to features and labels.\"\"\"",
            "\timage = tf.to_float(image) / 255.0",
            "\tlabel = tf.to_int64(label)",
            "\treturn {\"image\": image}, {\"label\": label}",
            ""
        ],
        "description": "MNIST dataset preprocessing and specifications"
    },
    "CIFAR10 dataset" : {
        "prefix": "tf:cifar10",
        "body": [
            "\"\"\"Cifar10 dataset preprocessing and specifications.\"\"\"",
            "",
            "from __future__ import absolute_import",
            "from __future__ import division",
            "from __future__ import print_function",
            "",
            "import os",
            "import tarfile",
            "import numpy as np",
            "from six.moves import cPickle",
            "from six.moves import urllib",
            "import tensorflow as tf",
            "",
            "REMOTE_URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"",
            "LOCAL_DIR = os.path.join(\"data/cifar10/\")",
            "ARCHIVE_NAME = \"cifar-10-python.tar.gz\"",
            "DATA_DIR = \"cifar-10-batches-py/\"",
            "TRAIN_BATCHES = [\"data_batch_%d\" % (i + 1) for i in range(5)]",
            "TEST_BATCHES = [\"test_batch\"]",
            "",
            "IMAGE_SIZE = 32",
            "NUM_CLASSES = 10",
            "",
            "def get_params():",
            "\t\"\"\"Return dataset parameters.\"\"\"",
            "\treturn {",
            "\t\t\"image_size\": IMAGE_SIZE,",
            "\t\t\"num_classes\": NUM_CLASSES,",
            "\t}",
            "",
            "def prepare():",
            "\t\"\"\"Download the cifar dataset.\"\"\"",
            "\tif not os.path.exists(LOCAL_DIR):",
            "\t\tos.makedirs(LOCAL_DIR)",
            "\tif not os.path.exists(LOCAL_DIR + ARCHIVE_NAME):",
            "\t\tprint(\"Downloading...\")",
            "\t\turllib.request.urlretrieve(REMOTE_URL, LOCAL_DIR + ARCHIVE_NAME)",
            "\tif not os.path.exists(LOCAL_DIR + DATA_DIR):",
            "\t\tprint(\"Extracting files...\")",
            "\t\ttar = tarfile.open(LOCAL_DIR + ARCHIVE_NAME)",
            "\t\ttar.extractall(LOCAL_DIR)",
            "\t\ttar.close()",
            "",
            "def read(split):",
            "\t\"\"\"Create an instance of the dataset object.\"\"\"",
            "\t\"\"\"An iterator that reads and returns images and labels from cifar.\"\"\"",
            "\tbatches = {",
            "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_BATCHES,",
            "\t\ttf.estimator.ModeKeys.EVAL: TEST_BATCHES",
            "\t}[split]",
            "",
            "\tall_images = []",
            "\tall_labels = []",
            "",
            "\tfor batch in batches:",
            "\t\twith open(\"%s%s%s\" % (LOCAL_DIR, DATA_DIR, batch), \"rb\") as fo:",
            "\t\t\tdict = cPickle.load(fo)",
            "\t\t\timages = np.array(dict[\"data\"])",
            "\t\t\tlabels = np.array(dict[\"labels\"])",
            "",
            "\t\t\tnum = images.shape[0]",
            "\t\t\timages = np.reshape(images, [num, 3, IMAGE_SIZE, IMAGE_SIZE])",
            "\t\t\timages = np.transpose(images, [0, 2, 3, 1])",
            "\t\t\tprint(\"Loaded %d examples.\" % num)",
            "",
            "\t\t\tall_images.append(images)",
            "\t\t\tall_labels.append(labels)",
            "",
            "\tall_images = np.concatenate(all_images)",
            "\tall_labels = np.concatenate(all_labels)",
            "",
            "\treturn tf.contrib.data.Dataset.from_tensor_slices((all_images, all_labels))",
            "",
            "def parse(image, label):",
            "\t\"\"\"Parse input record to features and labels.\"\"\"",
            "\timage = tf.to_float(image) / 255.0",
            "\timage = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])",
            "\treturn {\"image\": image}, {\"label\": label}",
            ""
        ],
        "description": "CIFAR10 dataset preprocessing and specifications"
    },
    "CIFAR100 dataset" : {
        "prefix": "tf:cifar100",
        "body": [
            "\"\"\"Cifar100 dataset preprocessing and specifications.\"\"\"",
            "",
            "from __future__ import absolute_import",
            "from __future__ import division",
            "from __future__ import print_function",
            "",
            "import os",
            "import tarfile",
            "import numpy as np",
            "from six.moves import cPickle",
            "from six.moves import urllib",
            "import tensorflow as tf",
            "",
            "REMOTE_URL = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"",
            "LOCAL_DIR = os.path.join(\"data/cifar100/\")",
            "ARCHIVE_NAME = \"cifar-100-python.tar.gz\"",
            "DATA_DIR = \"cifar-100-python/\"",
            "TRAIN_BATCHES = [\"train\"]",
            "TEST_BATCHES = [\"test\"]",
            "",
            "IMAGE_SIZE = 32",
            "NUM_CLASSES = 100",
            "",
            "def get_params():",
            "\t\"\"\"Return dataset parameters.\"\"\"",
            "\treturn {",
            "\t\t\"image_size\": IMAGE_SIZE,",
            "\t\t\"num_classes\": NUM_CLASSES,",
            "\t}",
            "",
            "def prepare():",
            "\t\"\"\"Download the cifar 100 dataset.\"\"\"",
            "\tif not os.path.exists(LOCAL_DIR):",
            "\t\tos.makedirs(LOCAL_DIR)",
            "\tif not os.path.exists(LOCAL_DIR + ARCHIVE_NAME):",
            "\t\tprint(\"Downloading...\")",
            "\t\turllib.request.urlretrieve(REMOTE_URL, LOCAL_DIR + ARCHIVE_NAME)",
            "\tif not os.path.exists(LOCAL_DIR + DATA_DIR):",
            "\t\tprint(\"Extracting files...\")",
            "\t\ttar = tarfile.open(LOCAL_DIR + ARCHIVE_NAME)",
            "\t\ttar.extractall(LOCAL_DIR)",
            "\t\ttar.close()",
            "",
            "def read(split):",
            "\t\"\"\"Create an instance of the dataset object.\"\"\"",
            "\tbatches = {",
            "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_BATCHES,",
            "\t\ttf.estimator.ModeKeys.EVAL: TEST_BATCHES",
            "\t}[split]",
            "",
            "\tall_images = []",
            "\tall_labels = []",
            "",
            "\tfor batch in batches:",
            "\t\twith open(\"%s%s%s\" % (LOCAL_DIR, DATA_DIR, batch), \"rb\") as fo:",
            "\t\t\tdict = cPickle.load(fo)",
            "\t\t\timages = np.array(dict[\"data\"])",
            "\t\t\tlabels = np.array(dict[\"fine_labels\"])",
            "",
            "\t\t\tnum = images.shape[0]",
            "\t\t\timages = np.reshape(images, [num, 3, IMAGE_SIZE, IMAGE_SIZE])",
            "\t\t\timages = np.transpose(images, [0, 2, 3, 1])",
            "\t\t\tprint(\"Loaded %d examples.\" % num)",
            "",
            "\t\t\tall_images.append(images)",
            "\t\t\tall_labels.append(labels)",
            "",
            "\tall_images = np.concatenate(all_images)",
            "\tall_labels = np.concatenate(all_labels)",
            "",
            "\treturn tf.contrib.data.Dataset.from_tensor_slices((all_images, all_labels))",
            "",
            "def parse(image, label):",
            "\t\"\"\"Parse input record to features and labels.\"\"\"",
            "\timage = tf.to_float(image) / 255.0",
            "\timage = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])",
            "\treturn {\"image\": image}, {\"label\": label}",
            ""
        ],
        "description": "CIFAR100 dataset preprocessing and specifications"
    },
    "CNN classifier": {
        "prefix": "tf:cnn-classifier",
        "body": [
            "\"\"\"Simple convolutional neural network classififer.\"\"\"",
            "",
            "from __future__ import absolute_import",
            "from __future__ import division",
            "from __future__ import print_function",
            "",
            "import tensorflow as tf",
            "",
            "FLAGS = tf.flags.FLAGS",
            "",
            "def get_params():",
            "\t\"\"\"Model params.\"\"\"",
            "\treturn {",
            "\t\t\"drop_rate\": 0.5",
            "\t}",
            "",
            "def model(features, labels, mode, params):",
            "\t\"\"\"CNN classifier model.\"\"\"",
            "\timages = features[\"image\"]",
            "\tlabels = labels[\"label\"]",
            "",
            "\ttf.summary.image(\"images\", images)",
            "",
            "\tdrop_rate = params.drop_rate if mode == tf.estimator.ModeKeys.TRAIN else 0.0",
            "",
            "\tfeatures = images",
            "\tfor i, filters in enumerate([32, 64, 128]):",
            "\t\tfeatures = tf.layers.conv2d(",
            "\t\t\tfeatures, filters=filters, kernel_size=3, padding=\"same\",",
            "\t\t\tname=\"conv_%d\" % (i + 1))",
            "\t\tfeatures = tf.layers.max_pooling2d(",
            "\t\t\tinputs=features, pool_size=2, strides=2, padding=\"same\",",
            "\t\t\tname=\"pool_%d\" % (i + 1))",
            "",
            "\tfeatures = tf.contrib.layers.flatten(features)",
            "",
            "\tfeatures = tf.layers.dropout(features, drop_rate)",
            "\tfeatures = tf.layers.dense(features, 512, name=\"dense_1\")",
            "",
            "\tfeatures = tf.layers.dropout(features, drop_rate)",
            "\tlogits = tf.layers.dense(features, params.num_classes, activation=None,",
            "\t\t\t\t\t\t\t name=\"dense_2\")",
            "",
            "\tpredictions = tf.argmax(logits, axis=1)",
            "",
            "\tloss = tf.losses.sparse_softmax_cross_entropy(",
            "\t\tlabels=labels, logits=logits)",
            "",
            "\treturn {\"predictions\": predictions}, loss",
            "",
            "def eval_metrics(unused_params):",
            "\t\"\"\"Eval metrics.\"\"\"",
            "\treturn {",
            "\t\t\"accuracy\": tf.contrib.learn.MetricSpec(tf.metrics.accuracy)",
            "\t}",
            ""
        ],
        "description": "A simple convolutional neural network classifier."
    }
}
